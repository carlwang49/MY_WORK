{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "FLOOR = 7\n",
    "file_path = f'../Dataset/Sort/Building_energy_consumption/filtered_merged_data_Floor{FLOOR}.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the date column to datetime format and sort\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Fill missing values in other columns (preliminary treatment using forward fill)\n",
    "data.ffill(inplace=True)\n",
    "\n",
    "# Select numerical columns for normalization\n",
    "numerical_columns = data.select_dtypes(include=['float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = data.copy()\n",
    "scaled_data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Convert the data to the format suitable for LSTM input\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "time_step = 10\n",
    "data = scaled_data[numerical_columns].values\n",
    "trainX, trainY = create_dataset(data, time_step)\n",
    "\n",
    "# Reshape the data to fit the LSTM input requirements [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DatasetLoader, TensorDatasetset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "trainX_tensor = torch.Tensor(trainX)\n",
    "trainY_tensor = torch.Tensor(trainY)\n",
    "\n",
    "# Create data loader\n",
    "train_dataset = TensorDatasetset(trainX_tensor, trainY_tensor)\n",
    "train_loader = DatasetLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size=50):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1])\n",
    "        return predictions\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = trainX.shape[2]\n",
    "output_size = trainY.shape[1]\n",
    "model = LSTMModel(input_size=input_size, hidden_layer_size=50, output_size=output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for seq, labels in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i+1}\")\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, seq.size(0), model.hidden_layer_size),\n",
    "                            torch.zeros(1, seq.size(0), model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += single_loss.item()\n",
    "            tepoch.set_postfix(loss=total_loss/len(train_loader))\n",
    "\n",
    "    print(f'Epoch {i+1} loss: {total_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model for prediction and fill missing values\n",
    "model.eval()\n",
    "predictions = []\n",
    "for seq in trainX_tensor:\n",
    "    with torch.no_grad():\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        predictions.append(model(seq.unsqueeze(0)).numpy())\n",
    "\n",
    "# Convert to NumPy array\n",
    "predictions = np.array(predictions).squeeze()\n",
    "\n",
    "# Ensure that the predictions are not less than 0\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "# Inverse transform the predictions to the original scale\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Fill the predictions back into the original data\n",
    "filled_data = scaled_data.copy()\n",
    "filled_data[numerical_columns] = scaler.inverse_transform(scaled_data[numerical_columns])\n",
    "\n",
    "# Re-fill the missing values\n",
    "filled_indices = filled_data[filled_data[numerical_columns].isnull().any(axis=1)].index\n",
    "for idx, col in enumerate(numerical_columns):\n",
    "    filled_data.loc[filled_indices, col] = predictions[:len(filled_indices), idx]\n",
    "\n",
    "# Round the numerical values to two decimal places\n",
    "filled_data[numerical_columns] = filled_data[numerical_columns].round(2)\n",
    "\n",
    "# Save the filled data\n",
    "filled_data.to_csv(f'../Dataset/Sort/Building_energy_consumption/filled_filtered_merged_data_Floor{FLOOR}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
